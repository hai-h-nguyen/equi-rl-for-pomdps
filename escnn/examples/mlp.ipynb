{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# E(n)-Equivariant Steerable CNNs  -  Equivariant MLPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from escnn import gspaces\n",
    "from escnn import nn\n",
    "from escnn import group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **escnn** library also supports MLPs equivariant to compact groups, which can be seen as a special case for $n=0$.\n",
    "This is done by replacing the convolution layers (e.g. [R3Conv](https://quva-lab.github.io/escnn/api/escnn.nn.html#r3conv)) with the [Linear](https://quva-lab.github.io/escnn/api/escnn.nn.html#linear) layer and by choosing the [no_base_space](https://quva-lab.github.io/escnn/api/escnn.gspaces.html#group-action-trivial-on-single-point) `GSpace` (e.g., instead of [rot3dOnR3](https://quva-lab.github.io/escnn/api/escnn.gspaces.html#escnn.gspaces.rot3dOnR3)). \n",
    "\n",
    "All other modules can be used in a similar way, e.g. batch-norm and non-linearities.\n",
    "\n",
    "\n",
    "Here, we provide an example with `G=SO(3)` and one with `G=O(2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO3MLP(nn.EquivariantModule):\n",
    "    \n",
    "    def __init__(self, n_classes=10):\n",
    "        \n",
    "        super(SO3MLP, self).__init__()\n",
    "        \n",
    "        # the model is equivariant to the group SO(3)\n",
    "        self.G = group.so3_group()\n",
    "        \n",
    "        # since we are building an MLP, there is no base-space\n",
    "        self.gspace = gspaces.no_base_space(self.G)\n",
    "        \n",
    "        # the input contains the coordinates of a point in the 3D space\n",
    "        self.in_type = self.gspace.type(self.G.standard_representation())\n",
    "        \n",
    "        # Layer 1\n",
    "        # We will use the representation of SO(3) acting on signals over a sphere, bandlimited to frequency 1\n",
    "        # To apply a point-wise non-linearity (e.g. ELU), we need to sample the spherical signals over a finite number of points.\n",
    "        # Note that this makes the equivariance only approximate.\n",
    "        # The representation of SO(3) on spherical signals is technically a quotient representation,\n",
    "        # identified by the subgroup of planar rotations, which has id=(False, -1) in our library\n",
    "        \n",
    "        # N.B.: the first this model is instantiated, the library computes numerically the spherical grids, which can take some time\n",
    "        # These grids are then cached on disk, so future calls should be considerably faster.\n",
    "        \n",
    "        activation1 = nn.QuotientFourierELU(\n",
    "            self.gspace,\n",
    "            subgroup_id=(False, -1),\n",
    "            channels=3, # specify the number of spherical signals in the output features\n",
    "            irreps=self.G.bl_sphere_representation(L=1).irreps, # include all frequencies up to L=1\n",
    "            grid=self.G.sphere_grid(type='thomson', N=16), # build a discretization of the sphere containing 16 equally distributed points            \n",
    "            inplace=True\n",
    "        )\n",
    "        \n",
    "        # map with an equivariant Linear layer to the input expected by the activation function, apply batchnorm and finally the activation\n",
    "        self.block1 = nn.SequentialModule(\n",
    "            nn.Linear(self.in_type, activation1.in_type),\n",
    "            nn.IIDBatchNorm1d(activation1.in_type),\n",
    "            activation1,\n",
    "        )\n",
    "        \n",
    "        # Repeat a similar process for a few layers\n",
    "        \n",
    "        # 8 spherical signals, bandlimited up to frequency 3\n",
    "        activation2 = nn.QuotientFourierELU(\n",
    "            self.gspace,\n",
    "            subgroup_id=(False, -1),\n",
    "            channels=8, # specify the number of spherical signals in the output features\n",
    "            irreps=self.G.bl_sphere_representation(L=3).irreps, # include all frequencies up to L=3\n",
    "            grid=self.G.sphere_grid(type='thomson', N=40), # build a discretization of the sphere containing 40 equally distributed points            \n",
    "            inplace=True\n",
    "        )\n",
    "        self.block2 = nn.SequentialModule(\n",
    "            nn.Linear(self.block1.out_type, activation2.in_type),\n",
    "            nn.IIDBatchNorm1d(activation2.in_type),\n",
    "            activation2,\n",
    "        )\n",
    "        \n",
    "        # 8 spherical signals, bandlimited up to frequency 3\n",
    "        activation3 = nn.QuotientFourierELU(\n",
    "            self.gspace,\n",
    "            subgroup_id=(False, -1),\n",
    "            channels=8, # specify the number of spherical signals in the output features\n",
    "            irreps=self.G.bl_sphere_representation(L=3).irreps, # include all frequencies up to L=3\n",
    "            grid=self.G.sphere_grid(type='thomson', N=40), # build a discretization of the sphere containing 40 equally distributed points            \n",
    "            inplace=True\n",
    "        )\n",
    "        self.block3 = nn.SequentialModule(\n",
    "            nn.Linear(self.block2.out_type, activation3.in_type),\n",
    "            nn.IIDBatchNorm1d(activation3.in_type),\n",
    "            activation3,\n",
    "        )\n",
    "        \n",
    "        # 5 spherical signals, bandlimited up to frequency 2\n",
    "        activation4 = nn.QuotientFourierELU(\n",
    "            self.gspace,\n",
    "            subgroup_id=(False, -1),\n",
    "            channels=5, # specify the number of spherical signals in the output features\n",
    "            irreps=self.G.bl_sphere_representation(L=2).irreps, # include all frequencies up to L=2\n",
    "            grid=self.G.sphere_grid(type='thomson', N=25), # build a discretization of the sphere containing 25 equally distributed points            \n",
    "            inplace=True\n",
    "        )\n",
    "        self.block4 = nn.SequentialModule(\n",
    "            nn.Linear(self.block3.out_type, activation4.in_type),\n",
    "            nn.IIDBatchNorm1d(activation4.in_type),\n",
    "            activation4,\n",
    "        )\n",
    "        \n",
    "        # Final linear layer mapping to the output features\n",
    "        # the output is a 5-dimensional vector transforming according to the Wigner-D matrix of frequency 2\n",
    "        self.out_type = self.gspace.type(self.G.irrep(2))\n",
    "        self.block5 = nn.Linear(self.block4.out_type, self.out_type)\n",
    "    \n",
    "    def forward(self, x: nn.GeometricTensor):\n",
    "        \n",
    "        # check the input has the right type\n",
    "        assert x.type == self.in_type\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "     \n",
    "        return x\n",
    "    \n",
    "    def evaluate_output_shape(self, input_shape: tuple):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) ==2, shape\n",
    "        assert shape[1] == self.in_type.size, shape\n",
    "        shape[1] = self.out_type.size\n",
    "        return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../escnn/group/groups/so3_utils.py:96: UserWarning: Gimbal lock detected. Setting third angle to zero since it is not possible to uniquely determine all angles.\n",
      "  return element.as_euler(param)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SO3MLP().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the equivariance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################################\n",
      "Outputs' magnitudes\n",
      "[0.1348 0.2192 0.2075 0.1902 0.2082 0.1977 0.1357 0.2424 0.227  0.1865]\n",
      "##########################################################################################\n",
      "Errors' magnitudes\n",
      "[0.0079 0.024  0.0232 0.0187 0.0244 0.0121 0.0131 0.0207 0.0155 0.0113]\n",
      "[0.0085 0.0183 0.0213 0.0175 0.0297 0.0146 0.0183 0.0167 0.0194 0.017 ]\n",
      "[0.0131 0.0202 0.0153 0.0148 0.0178 0.0074 0.0086 0.0144 0.0123 0.009 ]\n",
      "[0.0051 0.0132 0.0223 0.0143 0.0239 0.0216 0.0092 0.0153 0.0233 0.0091]\n",
      "[0.0128 0.0288 0.0185 0.0136 0.0153 0.0142 0.0135 0.0194 0.0206 0.0076]\n",
      "[0.0078 0.0197 0.0214 0.0153 0.0198 0.0132 0.0105 0.0145 0.0167 0.0083]\n",
      "[0.0072 0.0237 0.0202 0.0132 0.0248 0.0109 0.0104 0.0268 0.0167 0.0112]\n",
      "[0.005  0.0246 0.0243 0.0172 0.0259 0.016  0.0069 0.0187 0.0222 0.011 ]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=10000, precision=4, suppress=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "B = 10\n",
    "\n",
    "# generates B random points in 3D and wrap them in a GeometricTensor of the right type\n",
    "x = model.in_type(torch.randn(B, 3))\n",
    "\n",
    "\n",
    "print('##########################################################################################')\n",
    "with torch.no_grad():\n",
    "    y = model(x.to(device)).to('cpu')\n",
    "    print(\"Outputs' magnitudes\")\n",
    "    print(torch.linalg.norm(y.tensor, dim=1).numpy().reshape(-1))\n",
    "    print('##########################################################################################')\n",
    "    print(\"Errors' magnitudes\")\n",
    "    for r in range(8):\n",
    "        # sample a random rotation\n",
    "        g = model.G.sample()\n",
    "        \n",
    "        x_transformed = g @ x\n",
    "        x_transformed = x_transformed.to(device)\n",
    "\n",
    "        y_transformed = model(x_transformed).to('cpu')\n",
    "        \n",
    "        # verify that f(g@x) = g@f(x)=g@y\n",
    "        print(torch.linalg.norm(y_transformed.tensor - (g@y).tensor, dim=1).numpy().reshape(-1))        \n",
    "\n",
    "print('##########################################################################################')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO3MLPtensor(nn.EquivariantModule):\n",
    "    \n",
    "    def __init__(self, n_classes=10):\n",
    "        \n",
    "        super(SO3MLPtensor, self).__init__()\n",
    "        \n",
    "        # the model is equivariant to the group SO(3)\n",
    "        self.G = group.so3_group()\n",
    "        \n",
    "        # since we are building an MLP, there is no base-space\n",
    "        self.gspace = gspaces.no_base_space(self.G)\n",
    "        \n",
    "        # the input contains the coordinates of a point in the 3D space\n",
    "        in_repr = self.G.standard_representation()\n",
    "        self.in_type = self.gspace.type(in_repr)\n",
    "        \n",
    "        # Layer 1\n",
    "        # We will use the representation of SO(3) acting on signals over a sphere, bandlimited to frequency 2\n",
    "        # We use the tensor-product non-linearity, which is essentially a quadratic function.\n",
    "        \n",
    "        ttype = self.gspace.type(self.G.bl_sphere_representation(L=2))\n",
    "        activation1 = nn.TensorProductModule(self.in_type, ttype)\n",
    "        \n",
    "        # First we apply batch-norm and then the non-linearity. \n",
    "        # In the next blocks, we will also include a Linear layer.\n",
    "        self.block1 = nn.SequentialModule(\n",
    "            nn.IIDBatchNorm1d(activation1.in_type),\n",
    "            activation1,\n",
    "        )\n",
    "        \n",
    "        # Repeat a similar process for a few layers\n",
    "        \n",
    "        # input and output types must have the same number of fields (here, 8)\n",
    "        # the input one shouldn't have frequencies higher than the output of the previous block\n",
    "        activation2 = nn.TensorProductModule(\n",
    "            in_type = self.gspace.type(*[self.G.bl_sphere_representation(L=2)]*8),\n",
    "            out_type = self.gspace.type(*[self.G.bl_sphere_representation(L=3)]*8)    \n",
    "        )\n",
    "        self.block2 = nn.SequentialModule(\n",
    "            nn.Linear(self.block1.out_type, activation2.in_type),\n",
    "            nn.IIDBatchNorm1d(activation2.in_type),\n",
    "            activation2,\n",
    "        )\n",
    "        \n",
    "        activation3 = nn.TensorProductModule(\n",
    "            in_type = self.gspace.type(*[self.G.bl_sphere_representation(L=3)]*8),\n",
    "            out_type = self.gspace.type(*[self.G.bl_sphere_representation(L=3)]*8)    \n",
    "        )\n",
    "        self.block3 = nn.SequentialModule(\n",
    "            nn.Linear(self.block2.out_type, activation3.in_type),\n",
    "            nn.IIDBatchNorm1d(activation3.in_type),\n",
    "            activation3,\n",
    "        )\n",
    "        \n",
    "        activation4 = nn.TensorProductModule(\n",
    "            in_type = self.gspace.type(*[self.G.bl_sphere_representation(L=3)]*8),\n",
    "            out_type = self.gspace.type(*[self.G.irrep(2)]*8)    # the final layer only require frequency 2 features, so there is no point in generating other frequencies\n",
    "        )\n",
    "        self.block4 = nn.SequentialModule(\n",
    "            nn.Linear(self.block3.out_type, activation4.in_type),\n",
    "            nn.IIDBatchNorm1d(activation4.in_type),\n",
    "            activation4,\n",
    "        )\n",
    "        \n",
    "        # Final linear layer mapping to the output features\n",
    "        # the output is a 5-dimensional vector transforming according to the Wigner-D matrix of frequency 2\n",
    "        self.out_type = self.gspace.type(self.G.irrep(2))\n",
    "        self.block5 = nn.Linear(self.block4.out_type, self.out_type)\n",
    "    \n",
    "    def forward(self, x: nn.GeometricTensor):\n",
    "        \n",
    "        # check the input has the right type\n",
    "        assert x.type == self.in_type\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "     \n",
    "        return x\n",
    "    \n",
    "    def evaluate_output_shape(self, input_shape: tuple):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) ==2, shape\n",
    "        assert shape[1] == self.in_type.size, shape\n",
    "        shape[1] = self.out_type.size\n",
    "        return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SO3MLPtensor().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the equivariance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################################\n",
      "Outputs' magnitudes\n",
      "[133357.12        0.0907      0.      62198.625      80.044       0.    ]\n",
      "##########################################################################################\n",
      "Errors' magnitudes\n",
      "[0.18   0.     0.     0.0688 0.0002 0.    ]\n",
      "[0.0982 0.     0.     0.0999 0.0002 0.    ]\n",
      "[0.3416 0.     0.     0.1136 0.0001 0.    ]\n",
      "[0.1337 0.     0.     0.1165 0.0001 0.    ]\n",
      "[0.2221 0.     0.     0.11   0.     0.    ]\n",
      "[0.2336 0.     0.     0.076  0.0001 0.    ]\n",
      "[0.2423 0.     0.     0.0608 0.0001 0.    ]\n",
      "[0.1007 0.     0.     0.0967 0.0002 0.    ]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=10000, precision=4, suppress=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "B = 6\n",
    "\n",
    "# generates B random points in 3D and wrap them in a GeometricTensor of the right type\n",
    "x = model.in_type(torch.randn(B, 3))\n",
    "\n",
    "\n",
    "print('##########################################################################################')\n",
    "with torch.no_grad():\n",
    "    y = model(x.to(device)).to('cpu')\n",
    "    print(\"Outputs' magnitudes\")\n",
    "    print(torch.linalg.norm(y.tensor, dim=1).numpy().reshape(-1))\n",
    "    print('##########################################################################################')\n",
    "    print(\"Errors' magnitudes\")\n",
    "    for r in range(8):\n",
    "        # sample a random rotation\n",
    "        g = model.G.sample()\n",
    "        \n",
    "        x_transformed = g @ x\n",
    "        x_transformed = x_transformed.to(device)\n",
    "\n",
    "        y_transformed = model(x_transformed).to('cpu')\n",
    "        \n",
    "        # verify that f(g@x) = g@f(x)=g@y\n",
    "        print(torch.linalg.norm(y_transformed.tensor - (g@y).tensor, dim=1).numpy().reshape(-1))        \n",
    "\n",
    "print('##########################################################################################')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO2MLP(nn.EquivariantModule):\n",
    "    \n",
    "    def __init__(self, n_classes=10):\n",
    "        \n",
    "        super(SO2MLP, self).__init__()\n",
    "        \n",
    "        # the model is equivariant to the group O(2)\n",
    "        self.G = group.so2_group()\n",
    "        \n",
    "        # since we are building an MLP, there is no base-space\n",
    "        self.gspace = gspaces.no_base_space(self.G)\n",
    "        \n",
    "        # the input contains the coordinates of a point in the 2D space\n",
    "        self.in_type = self.gspace.type(self.G.standard_representation())\n",
    "        \n",
    "        # Layer 1\n",
    "        # We will use the regular representation of SO(2) acting on signals over SO(2) itself, bandlimited to frequency 1\n",
    "        # Most of the comments on the previous SO(3) network apply here as well\n",
    "       \n",
    "        activation1 = nn.FourierELU(\n",
    "            self.gspace,\n",
    "            channels=3, # specify the number of signals in the output features\n",
    "            irreps=self.G.bl_regular_representation(L=1).irreps, # include all frequencies up to L=1\n",
    "            inplace=True,\n",
    "            # the following kwargs are used to build a discretization of the circle containing 6 equally distributed points\n",
    "            type='regular', N=6,   \n",
    "        )\n",
    "        \n",
    "        # map with an equivariant Linear layer to the input expected by the activation function, apply batchnorm and finally the activation\n",
    "        self.block1 = nn.SequentialModule(\n",
    "            nn.Linear(self.in_type, activation1.in_type),\n",
    "            nn.IIDBatchNorm1d(activation1.in_type),\n",
    "            activation1,\n",
    "        )\n",
    "        \n",
    "        # Repeat a similar process for a few layers\n",
    "        \n",
    "        # 8 signals, bandlimited up to frequency 3\n",
    "        activation2 = nn.FourierELU(\n",
    "            self.gspace,\n",
    "            channels=8, # specify the number of signals in the output features\n",
    "            irreps=self.G.bl_regular_representation(L=3).irreps, # include all frequencies up to L=3\n",
    "            inplace=True,\n",
    "            # the following kwargs are used to build a discretization of the circle containing 16 equally distributed points\n",
    "            type='regular', N=16,\n",
    "        )\n",
    "        self.block2 = nn.SequentialModule(\n",
    "            nn.Linear(self.block1.out_type, activation2.in_type),\n",
    "            nn.IIDBatchNorm1d(activation2.in_type),\n",
    "            activation2,\n",
    "        )\n",
    "        \n",
    "        # 8 signals, bandlimited up to frequency 3\n",
    "        activation3 = nn.FourierELU(\n",
    "            self.gspace,\n",
    "            channels=8, # specify the number of signals in the output features\n",
    "            irreps=self.G.bl_regular_representation(L=3).irreps, # include all frequencies up to L=3\n",
    "            inplace=True,\n",
    "            # the following kwargs are used to build a discretization of the circle containing 16 equally distributed points\n",
    "            type='regular', N=16,\n",
    "        )\n",
    "        self.block3 = nn.SequentialModule(\n",
    "            nn.Linear(self.block2.out_type, activation3.in_type),\n",
    "            nn.IIDBatchNorm1d(activation3.in_type),\n",
    "            activation3,\n",
    "        )\n",
    "        \n",
    "        # 5 signals, bandlimited up to frequency 2\n",
    "        activation4 = nn.FourierELU(\n",
    "            self.gspace,\n",
    "            channels=5, # specify the number of signals in the output features\n",
    "            irreps=self.G.bl_regular_representation(L=2).irreps, # include all frequencies up to L=2\n",
    "            inplace=True,\n",
    "            # the following kwargs are used to build a discretization of the circle containing 12 equally distributed points\n",
    "            type='regular', N=12,\n",
    "        )\n",
    "        self.block4 = nn.SequentialModule(\n",
    "            nn.Linear(self.block3.out_type, activation4.in_type),\n",
    "            nn.IIDBatchNorm1d(activation4.in_type),\n",
    "            activation4,\n",
    "        )\n",
    "        \n",
    "        # Final linear layer mapping to the output features\n",
    "        # the output is a 2-dimensional vector rotating with frequency 2\n",
    "        self.out_type = self.gspace.type(self.G.irrep(2))\n",
    "        self.block5 = nn.Linear(self.block4.out_type, self.out_type)\n",
    "    \n",
    "    def forward(self, x: nn.GeometricTensor):\n",
    "        \n",
    "        # check the input has the right type\n",
    "        assert x.type == self.in_type\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "     \n",
    "        return x\n",
    "    \n",
    "    def evaluate_output_shape(self, input_shape: tuple):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) ==2, shape\n",
    "        assert shape[1] == self.in_type.size, shape\n",
    "        shape[1] = self.out_type.size\n",
    "        return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SO2MLP().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the equivariance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################################\n",
      "Outputs' magnitudes\n",
      "[0.115  0.1023 0.014  0.0306 0.0657 0.0292 0.0484 0.028  0.033  0.026 ]\n",
      "##########################################################################################\n",
      "Errors' magnitudes\n",
      "[0.0014 0.0004 0.     0.0001 0.0005 0.0001 0.0002 0.0001 0.0001 0.    ]\n",
      "[0.0018 0.0014 0.     0.0001 0.0003 0.0001 0.0004 0.0002 0.0001 0.0001]\n",
      "[0.0027 0.0006 0.0001 0.0002 0.0005 0.0002 0.0004 0.     0.0002 0.0002]\n",
      "[0.0026 0.0011 0.0001 0.0002 0.0004 0.0002 0.0005 0.0001 0.0002 0.0002]\n",
      "[0.001  0.001  0.     0.0001 0.0005 0.0001 0.0001 0.     0.0001 0.0001]\n",
      "[0.0016 0.001  0.0001 0.0001 0.0005 0.0002 0.0002 0.     0.0002 0.0001]\n",
      "[0.0016 0.0007 0.     0.0001 0.0006 0.     0.0002 0.0002 0.0001 0.    ]\n",
      "[0.0019 0.0012 0.     0.0001 0.0004 0.0001 0.0004 0.0002 0.0001 0.0001]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=10000, precision=4, suppress=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "B = 10\n",
    "\n",
    "# generates B random points in 2D and wrap them in a GeometricTensor of the right type\n",
    "x = model.in_type(torch.randn(B, 2))\n",
    "\n",
    "\n",
    "print('##########################################################################################')\n",
    "with torch.no_grad():\n",
    "    y = model(x.to(device)).to('cpu')\n",
    "    print(\"Outputs' magnitudes\")\n",
    "    print(torch.linalg.norm(y.tensor, dim=1).numpy().reshape(-1))\n",
    "    print('##########################################################################################')\n",
    "    print(\"Errors' magnitudes\")\n",
    "    for r in range(8):\n",
    "        # sample a random rotation\n",
    "        g = model.G.sample()\n",
    "        \n",
    "        x_transformed = g @ x\n",
    "        x_transformed = x_transformed.to(device)\n",
    "\n",
    "        y_transformed = model(x_transformed).to('cpu')\n",
    "        \n",
    "        # verify that f(g@x) = g@f(x)=g@y\n",
    "        print(torch.linalg.norm(y_transformed.tensor - (g@y).tensor, dim=1).numpy().reshape(-1))\n",
    "        \n",
    "\n",
    "print('##########################################################################################')\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
